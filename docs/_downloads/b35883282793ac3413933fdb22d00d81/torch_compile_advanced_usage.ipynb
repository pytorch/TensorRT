{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Torch Compile Advanced Usage\n\nThis interactive script is intended as an overview of the process by which `torch_tensorrt.compile(..., ir=\"torch_compile\", ...)` works, and how it integrates with the `torch.compile` API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Model Definition\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch_tensorrt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We begin by defining a model\nclass Model(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.relu = torch.nn.ReLU()\n\n    def forward(self, x: torch.Tensor, y: torch.Tensor):\n        x_out = self.relu(x)\n        y_out = self.relu(y)\n        x_y_out = x_out + y_out\n        return torch.mean(x_y_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compilation with `torch.compile` Using Default Settings\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define sample float inputs and initialize model\nsample_inputs = [torch.rand((5, 7)).cuda(), torch.rand((5, 7)).cuda()]\nmodel = Model().eval().cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Next, we compile the model using torch.compile\n# For the default settings, we can simply call torch.compile\n# with the backend \"torch_tensorrt\", and run the model on an\n# input to cause compilation, as so:\noptimized_model = torch.compile(model, backend=\"torch_tensorrt\", dynamic=False)\noptimized_model(*sample_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compilation with `torch.compile` Using Custom Settings\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# First, we use Torch utilities to clean up the workspace\n# after the previous compile invocation\ntorch._dynamo.reset()\n\n# Define sample half inputs and initialize model\nsample_inputs_half = [\n    torch.rand((5, 7)).half().cuda(),\n    torch.rand((5, 7)).half().cuda(),\n]\nmodel_half = Model().eval().cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# If we want to customize certain options in the backend,\n# but still use the torch.compile call directly, we can provide\n# custom options to the backend via the \"options\" keyword\n# which takes in a dictionary mapping options to values.\n#\n# For accepted backend options, see the CompilationSettings dataclass:\n# py/torch_tensorrt/dynamo/_settings.py\nbackend_kwargs = {\n    \"enabled_precisions\": {torch.half},\n    \"debug\": True,\n    \"min_block_size\": 2,\n    \"torch_executed_ops\": {\"torch.ops.aten.sub.Tensor\"},\n    \"optimization_level\": 4,\n    \"use_python_runtime\": False,\n}\n\n# Run the model on an input to cause compilation, as so:\noptimized_model_custom = torch.compile(\n    model_half,\n    backend=\"torch_tensorrt\",\n    options=backend_kwargs,\n    dynamic=False,\n)\noptimized_model_custom(*sample_inputs_half)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Finally, we use Torch utilities to clean up the workspace\ntorch._dynamo.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cuda Driver Error Note\n\nOccasionally, upon exiting the Python runtime after Dynamo compilation with `torch_tensorrt`,\none may encounter a Cuda Driver Error. This issue is related to https://github.com/NVIDIA/TensorRT/issues/2052\nand can be resolved by wrapping the compilation/inference in a function and using a scoped call, as in::\n\n      if __name__ == '__main__':\n          compile_engine_and_infer()\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}