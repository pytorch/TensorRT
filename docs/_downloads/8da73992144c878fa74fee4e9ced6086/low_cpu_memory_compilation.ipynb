{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Low CPU Memory Compilation Example\n\nThis example demonstrates compiling a model with a bounded CPU (host) memory\nbudget using Torch-TensorRT Dynamo. Limiting host RAM use is helpful on\nmemory-constrained machines or when compiling very large models.\n\nKey notes:\n- The toy model below has roughly 430 MB of parameters. We set the CPU\n  memory budget to 2 GiB. At compile time, only about 900 MB of host RAM\n  may remain available. We expect at most 403 * 4 = 1612 MB of memory to be used by the model.\n  So the model is partitioned into two subgraphs to fit the memory budget.\n\n- Performance impact varies by model. When the number of TensorRT engines\n  created is small, the impact is typically minimal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch_tensorrt as torchtrt\nfrom torch_tensorrt.dynamo.conversion import CompilationSettings\n\n\nclass net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Intentionally large layers to stress host memory during compilation.\n        self.conv1 = nn.Conv2d(1024, 4096, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(4096)\n        self.conv2 = nn.Conv2d(4096, 1024, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(1024)\n        self.fc1 = nn.Linear(1024 * 56 * 56, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, (2, 2))\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, (2, 2))\n        x = torch.flatten(x, 1)\n        return self.fc1(x)\n\n\nmodel = net().eval()\nmodel.to(\"cuda\")\ninputs = [torch.randn((1, 1024, 224, 224)).to(\"cuda\")]\n\nenabled_precisions = {torch.float}\nuse_python_runtime = False\n\ncompilation_options = {\n    \"use_python_runtime\": use_python_runtime,\n    \"enabled_precisions\": enabled_precisions,\n    \"min_block_size\": 1,\n    \"immutable_weights\": True,\n    \"reuse_cached_engines\": False,\n    \"enable_resource_partitioning\": True,\n    \"cpu_memory_budget\": 2 * 1024 * 1024 * 1024,  # 2 GiB in bytes\n}\n\nsettings = CompilationSettings(**compilation_options)\nwith torchtrt.dynamo.Debugger(\n    log_level=\"debug\",\n    logging_dir=\"/home/profile/logging/moe\",\n    engine_builder_monitor=False,\n):\n    exp_program = torch.export.export(model, tuple(inputs))\n    trt_gm = torchtrt.dynamo.compile(\n        exp_program,\n        inputs=inputs,\n        **compilation_options,\n    )\n\n    # Expect two back-to-back TensorRT engines due to partitioning under the memory budget.\n    print(trt_gm)\n\n\n\"\"\"\nYou should be able to see two back-to-back TensorRT engines in the graph\n\nGraph Structure:\n\n   Inputs: List[Tensor: (1, 1024, 224, 224)@float32]\n    ...\n    TRT Engine #1 - Submodule name: _run_on_acc_0_resource_split_0\n     Engine Inputs: List[Tensor: (1, 1024, 224, 224)@float32]\n     Number of Operators in Engine: 9\n     Engine Outputs: List[Tensor: (1, 1024, 112, 112)@float32]\n    ...\n    TRT Engine #2 - Submodule name: _run_on_acc_0_resource_split_1\n     Engine Inputs: List[Tensor: (1, 1024, 112, 112)@float32]\n     Number of Operators in Engine: 3\n     Engine Outputs: List[Tensor: (1, 10)@float32]\n    ...\n   Outputs: List[Tensor: (1, 10)@float32]\n\n  ------------------------- Aggregate Stats -------------------------\n\n   Average Number of Operators per TRT Engine: 6.0\n   Most Operators in a TRT Engine: 9\n\n  ********** Recommendations **********\n\n   - For minimal graph segmentation, select min_block_size=9 which would generate 1 TRT engine(s)\n   - For moderate graph segmentation, select min_block_size=6 which would generate 1 TRT engine(s)\n   - The current level of graph segmentation is equivalent to selecting min_block_size=3 which generates 2 TRT engine(s)\nGraphModule(\n  (_run_on_acc_0_resource_split_0): TorchTensorRTModule()\n  (_run_on_acc_0_resource_split_1): TorchTensorRTModule()\n)\n\n\n\ndef forward(self, x):\n    x, = fx_pytree.tree_flatten_spec(([x], {}), self._in_spec)\n    _run_on_acc_0_resource_split_0 = self._run_on_acc_0_resource_split_0(x);  x = None\n    _run_on_acc_0_resource_split_1 = self._run_on_acc_0_resource_split_1(_run_on_acc_0_resource_split_0);  _run_on_acc_0_resource_split_0 = None\n    return pytree.tree_unflatten((_run_on_acc_0_resource_split_1,), self._out_spec)\n)\n\"\"\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}