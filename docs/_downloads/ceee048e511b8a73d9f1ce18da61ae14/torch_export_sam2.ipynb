{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Compiling SAM2 using the dynamo backend\n\nThis example illustrates the state of the art model [Segment Anything Model 2 (SAM2)](https://arxiv.org/pdf/2408.00714) optimized using\nTorch-TensorRT.\n\n**Segment Anything Model 2** is a foundation model towards solving promptable visual segmentation in images and videos.\nInstall the following dependencies before compilation\n\n```python\npip install -r requirements.txt\n```\nCertain custom modifications are required to ensure the model is exported successfully. To apply these changes, please install SAM2 using the [following fork](https://github.com/chohk88/sam2/tree/torch-trt) ([Installation instructions](https://github.com/chohk88/sam2/tree/torch-trt?tab=readme-ov-file#installation))\n\nIn the custom SAM2 fork, the following modifications have been applied to remove graph breaks and enhance latency performance, ensuring a more efficient Torch-TRT conversion:\n\n- **Consistent Data Types:** Preserves input tensor dtypes, removing forced FP32 conversions.\n- **Masked Operations:** Uses mask-based indexing instead of directly selecting data, improving Torch-TRT compatibility.\n- **Safe Initialization:** Initializes tensors conditionally rather than concatenating to empty tensors.\n- **Standard Functions:** Avoids special contexts and custom LayerNorm, relying on built-in PyTorch functions for better stability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import the following libraries\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch_tensorrt\nfrom PIL import Image\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\nfrom sam_components import SAM2FullModel\n\nmatplotlib.use(\"Agg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the SAM2 model\nLoad the ``facebook/sam2-hiera-large`` pretrained model using ``SAM2ImagePredictor`` class.\n``SAM2ImagePredictor`` provides utilities to preprocess images, store image features (via ``set_image`` function)\nand predict the masks (via ``predict`` function)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predictor = SAM2ImagePredictor.from_pretrained(\"facebook/sam2-hiera-large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To ensure we export the entire model (image encoder and mask predictor) components successfully, we create a\nstandalone module ``SAM2FullModel`` which uses these utilities from ``SAM2ImagePredictor`` class.\n``SAM2FullModel`` performs feature extraction and mask prediction in a single step instead of two step process of\n``SAM2ImagePredictor`` (set_image and predict functions)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class SAM2FullModel(torch.nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.image_encoder = model.forward_image\n        self._prepare_backbone_features = model._prepare_backbone_features\n        self.directly_add_no_mem_embed = model.directly_add_no_mem_embed\n        self.no_mem_embed = model.no_mem_embed\n        self._features = None\n\n        self.prompt_encoder = model.sam_prompt_encoder\n        self.mask_decoder = model.sam_mask_decoder\n\n        self._bb_feat_sizes = [(256, 256), (128, 128), (64, 64)]\n\n    def forward(self, image, point_coords, point_labels):\n        backbone_out = self.image_encoder(image)\n        _, vision_feats, _, _ = self._prepare_backbone_features(backbone_out)\n\n        if self.directly_add_no_mem_embed:\n            vision_feats[-1] = vision_feats[-1] + self.no_mem_embed\n\n        feats = [\n            feat.permute(1, 2, 0).view(1, -1, *feat_size)\n            for feat, feat_size in zip(vision_feats[::-1], self._bb_feat_sizes[::-1])\n        ][::-1]\n        features = {\"image_embed\": feats[-1], \"high_res_feats\": feats[:-1]}\n\n        high_res_features = [\n            feat_level[-1].unsqueeze(0) for feat_level in features[\"high_res_feats\"]\n        ]\n\n        sparse_embeddings, dense_embeddings = self.prompt_encoder(\n            points=(point_coords, point_labels), boxes=None, masks=None\n        )\n\n        low_res_masks, iou_predictions, _, _ = self.mask_decoder(\n            image_embeddings=features[\"image_embed\"][-1].unsqueeze(0),\n            image_pe=self.prompt_encoder.get_dense_pe(),\n            sparse_prompt_embeddings=sparse_embeddings,\n            dense_prompt_embeddings=dense_embeddings,\n            multimask_output=True,\n            repeat_image=point_coords.shape[0] > 1,\n            high_res_features=high_res_features,\n        )\n\n        out = {\"low_res_masks\": low_res_masks, \"iou_predictions\": iou_predictions}\n        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize the SAM2 model with pretrained weights\nInitialize the ``SAM2FullModel`` with the pretrained weights. Since we already initialized\n``SAM2ImagePredictor``, we can directly use the model from it (``predictor.model``). We cast the model\nto FP16 precision for faster performance.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "encoder = predictor.model.eval().cuda()\nsam_model = SAM2FullModel(encoder.half()).eval().cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load a sample image provided in the repository.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_image = Image.open(\"./truck.jpg\").convert(\"RGB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load an input image\nHere's the input image we are going to use\n\n<img src=\"file://./truck.jpg\">\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_image = Image.open(\"./truck.jpg\").convert(\"RGB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In addition to the input image, we also provide prompts as inputs which are\nused to predict the masks. The prompts can be a box, point as well as masks from\nprevious iteration of prediction. We use a point as a prompt in this demo similar to\nthe [original notebook in the SAM2 repository](https://github.com/facebookresearch/sam2/blob/main/notebooks/image_predictor_example.ipynb)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing components\nThe following functions implement preprocessing components which apply transformations on the input image\nand transform given point coordinates. We use the SAM2Transforms available via the SAM2ImagePredictor class.\nTo read more about the transforms, refer to https://github.com/facebookresearch/sam2/blob/main/sam2/utils/transforms.py\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def preprocess_inputs(image, predictor):\n    w, h = image.size\n    orig_hw = [(h, w)]\n    input_image = predictor._transforms(np.array(image))[None, ...].to(\"cuda:0\")\n\n    point_coords = torch.tensor([[500, 375]], dtype=torch.float).to(\"cuda:0\")\n    point_labels = torch.tensor([1], dtype=torch.int).to(\"cuda:0\")\n\n    point_coords = torch.as_tensor(\n        point_coords, dtype=torch.float, device=predictor.device\n    )\n    unnorm_coords = predictor._transforms.transform_coords(\n        point_coords, normalize=True, orig_hw=orig_hw[0]\n    )\n    labels = torch.as_tensor(point_labels, dtype=torch.int, device=predictor.device)\n    if len(unnorm_coords.shape) == 2:\n        unnorm_coords, labels = unnorm_coords[None, ...], labels[None, ...]\n\n    input_image = input_image.half()\n    unnorm_coords = unnorm_coords.half()\n\n    return (input_image, unnorm_coords, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post Processing components\nThe following functions implement postprocessing components which include plotting and visualizing masks and points.\nWe use the SAM2Transforms to post process these masks and sort them via confidence score.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def postprocess_masks(out, predictor, image):\n    \"\"\"Postprocess low-resolution masks and convert them for visualization.\"\"\"\n    orig_hw = (image.size[1], image.size[0])  # (height, width)\n    masks = predictor._transforms.postprocess_masks(out[\"low_res_masks\"], orig_hw)\n    masks = (masks > 0.0).squeeze(0).cpu().numpy()\n    scores = out[\"iou_predictions\"].squeeze(0).cpu().numpy()\n    sorted_indices = np.argsort(scores)[::-1]\n    return masks[sorted_indices], scores[sorted_indices]\n\n\ndef show_mask(mask, ax, random_color=False, borders=True):\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n    h, w = mask.shape[-2:]\n    mask = mask.astype(np.uint8)\n    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    if borders:\n        import cv2\n\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n        # Try to smooth contours\n        contours = [\n            cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours\n        ]\n        mask_image = cv2.drawContours(\n            mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2\n        )\n    ax.imshow(mask_image)\n\n\ndef show_points(coords, labels, ax, marker_size=375):\n    pos_points = coords[labels == 1]\n    neg_points = coords[labels == 0]\n    ax.scatter(\n        pos_points[:, 0],\n        pos_points[:, 1],\n        color=\"green\",\n        marker=\"*\",\n        s=marker_size,\n        edgecolor=\"white\",\n        linewidth=1.25,\n    )\n    ax.scatter(\n        neg_points[:, 0],\n        neg_points[:, 1],\n        color=\"red\",\n        marker=\"*\",\n        s=marker_size,\n        edgecolor=\"white\",\n        linewidth=1.25,\n    )\n\n\ndef visualize_masks(\n    image, masks, scores, point_coords, point_labels, title_prefix=\"\", save=True\n):\n    \"\"\"Visualize and save masks overlaid on the original image.\"\"\"\n    for i, (mask, score) in enumerate(zip(masks, scores)):\n        plt.figure(figsize=(10, 10))\n        plt.imshow(image)\n        show_mask(mask, plt.gca())\n        show_points(point_coords, point_labels, plt.gca())\n        plt.title(f\"{title_prefix} Mask {i + 1}, Score: {score:.3f}\", fontsize=18)\n        plt.axis(\"off\")\n        plt.savefig(f\"{title_prefix}_output_mask_{i + 1}.png\")\n        plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocess the inputs\nPreprocess the inputs. In the following snippet, ``torchtrt_inputs`` contains (input_image, unnormalized_coordinates and labels)\nThe unnormalized_coordinates is the representation of the point and the label (= 1 in this demo) represents foreground point.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torchtrt_inputs = preprocess_inputs(input_image, predictor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Torch-TensorRT compilation\nExport the model in non-strict mode and perform Torch-TensorRT compilation in FP16 precision.\nWe enable FP32 matmul accumulation using ``use_fp32_acc=True`` to preserve accuracy with the original Pytorch model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "exp_program = torch.export.export(sam_model, torchtrt_inputs, strict=False)\ntrt_model = torch_tensorrt.dynamo.compile(\n    exp_program,\n    inputs=torchtrt_inputs,\n    min_block_size=1,\n    enabled_precisions={torch.float16},\n    use_fp32_acc=True,\n)\ntrt_out = trt_model(*torchtrt_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Output visualization\nPost process the outputs of Torch-TensorRT and visualize the masks using the post processing\ncomponents provided above. The outputs should be stored in your current directory.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trt_masks, trt_scores = postprocess_masks(trt_out, predictor, input_image)\nvisualize_masks(\n    input_image,\n    trt_masks,\n    trt_scores,\n    torch.tensor([[500, 375]]),\n    torch.tensor([1]),\n    title_prefix=\"Torch-TRT\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The predicted masks are as shown below\n   .. image:: sam_mask1.png\n      :width: 50%\n\n   .. image:: sam_mask2.png\n      :width: 50%\n\n   .. image:: sam_mask3.png\n      :width: 50%\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n- [SAM 2: Segment Anything in Images and Videos](https://arxiv.org/pdf/2408.00714)\n- [SAM 2 Github Repository](https://github.com/facebookresearch/sam2/tree/main)\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}