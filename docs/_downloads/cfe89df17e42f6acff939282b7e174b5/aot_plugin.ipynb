{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nAutomatically Generate a TensorRT AOT Plugin\n===================================================================\nWe are going to demonstrate how to automatically generate a plugin for a custom kernel using Torch-TensorRT using\nthe new Python based plugin system in TensorRT 10.7.\n\nTorch-TensorRT supports falling back to PyTorch implementations of operations in the case that Torch-TensorRT\ndoes not know how to compile them in TensorRT. However, this comes at the cost of a graph break and will reduce the performance of the model.\nThe easiest way to fix lack of support for ops is by adding a decomposition (see:\n[Writing lowering passes for the Dynamo frontend](https://pytorch.org/TensorRT/contributors/writing_dynamo_aten_lowering_passes.html)) - which defines the operator\nin terms of PyTorch ops that are supported in Torch-TensorRT or a converter (see:\n[Writing converters for the Dynamo frontend](https://pytorch.org/TensorRT/contributors/dynamo_converters.html)) - which defines the operator in terms of TensorRT operators.\n\nIn some cases there isn't a great way to do either of these, perhaps because the operator is a custom kernel that is not part of standard PyTorch or\nTensorRT cannot support it natively.\n\nFor these cases, it is possible to use a TensorRT plugin to replace the operator **inside** the TensorRT engine, thereby avoiding\nthe performance and resource overhead from a graph break.\n\nPreviously this involved a complex process in not only building a performant kernel but setting it up to run in TensorRT (see: [Using Custom Kernels within TensorRT Engines with Torch-TensorRT](https://pytorch.org/TensorRT/tutorials/_rendered_examples/dynamo/custom_kernel_plugins.html)).\nAs of TensorRT 10.7, there is a new Python native plugin system which greatly streamlines this process. This\nplugin system also allows Torch-TensorRT to automatically generate the necessary conversion code to convert the\noperation in PyTorch to TensorRT.\n\nIn addition, Torch-TensorRT provides automatic generation of TensorRT plugin feature (see: [Automatically Generate a Plugin for a Custom Kernel](https://docs.pytorch.org/TensorRT/tutorials/_rendered_examples/dynamo/auto_generate_plugins.html)).\nHowever, the above methods generates a JIT plugin that might not satisfy user's performance requirements.\nTo support that, Torch-TensorRT provides auto generation of TensorRT AOT Plugin which raps a function to define an Ahead-of-Time (AOT) implementation for a plugin already registered.\nThis provides a performance boost comparing to JIT plugin.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import argparse\nfrom typing import Tuple, Union\n\nimport tensorrt as trt\nimport tensorrt.plugin as trtp\nimport torch\nimport torch_tensorrt\nimport triton\nimport triton.language as tl\n\ntrt_logger = trt.Logger(trt.Logger.VERBOSE)\n\n\n@triton.jit\ndef add_one_kernel(x_ptr, n_elements, y_ptr, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    output = x + 1\n    tl.store(y_ptr + offsets, output, mask=mask)\n\n\n@torch.library.custom_op(\"my::add_one\", mutates_args=())  # type: ignore[misc]\ndef add_one(X: torch.Tensor) -> torch.Tensor:\n    # Ensure the tensors are on the GPU\n    assert X.is_cuda\n\n    # Create output tensor\n    Y = torch.empty_like(X)\n\n    # Define block size\n    BLOCK_SIZE = 256\n\n    # Grid of programs\n    grid = lambda meta: (triton.cdiv(X.numel(), meta[\"BLOCK_SIZE\"]),)\n\n    # Launch the kernel\n    add_one_kernel[grid](X, X.numel(), Y, BLOCK_SIZE=BLOCK_SIZE)\n\n    return Y\n\n\n@torch.library.register_fake(\"my::add_one\")\ndef _(X: torch.Tensor) -> torch.Tensor:\n    return X\n\n\n@trtp.register(\"my::add_one\")\ndef add_plugin_desc(X: trtp.TensorDesc) -> Tuple[trtp.TensorDesc]:\n    return X.like()\n\n\n@trtp.aot_impl(\"my::add_one\")\ndef add_plugin_aot_impl(\n    X: trtp.TensorDesc, outputs: Tuple[trtp.TensorDesc], tactic: int\n) -> Tuple[\n    Union[str, bytes], Union[str, bytes], trtp.KernelLaunchParams, trtp.SymExprs\n]:\n    type_str = \"fp32\" if X.dtype == trt.float32 else \"fp16\"\n\n    block_size = 256\n    src = triton.compiler.ASTSource(\n        fn=add_one_kernel,\n        signature={\n            \"x_ptr\": f\"*{type_str}\",\n            \"n_elements\": \"i32\",\n            \"y_ptr\": f\"*{type_str}\",\n            \"BLOCK_SIZE\": \"constexpr\",\n        },\n        constants={\n            \"BLOCK_SIZE\": block_size,\n        },\n    )\n\n    compiled_kernel = triton.compile(src)\n\n    N = X.shape_expr.numel()\n    launch_params = trtp.KernelLaunchParams()\n\n    # grid dims\n    launch_params.grid_x = trtp.cdiv(N, block_size)\n    # block dims\n    launch_params.block_x = compiled_kernel.metadata.num_warps * 32\n    # shared memory\n    launch_params.shared_mem = compiled_kernel.metadata.shared\n\n    extra_args = trtp.SymIntExprs(1)\n    extra_args[0] = trtp.SymInt32(N)\n\n    return (\n        compiled_kernel.metadata.name,\n        compiled_kernel.asm[\"ptx\"],\n        launch_params,\n        extra_args,\n    )\n\n\ntorch_tensorrt.dynamo.conversion.plugins.generate_plugin_converter(\n    \"my::add_one\",\n    supports_dynamic_shapes=False,\n    requires_output_allocator=False,\n    use_aot_if_available=True,\n)\n\n\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        res = torch.ops.my.add_one.default(X)\n\n        return res\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--aot\", action=\"store_true\", help=\"Try to use AOT compilation\", default=False\n    )\n    args = parser.parse_args()\n\n    my_model = MyModel().to(\"cuda\").eval()\n    m = torch.full((64, 64), 2, device=\"cuda\", dtype=torch.float)\n\n    assert my_model(X=m)[0][0] == 3.0\n\n    with torch_tensorrt.logging.debug():\n        trt_inputs = [m]\n        model_trt = torch_tensorrt.compile(\n            my_model,\n            inputs=trt_inputs,\n            min_block_size=1,\n        )\n        print(\"Model compiled successfully!\")\n        print(\"Running inference with compiled model...\")\n        with torch.no_grad():\n            for i in range(10):\n                res = model_trt(m)\n                assert torch.allclose(res, my_model(m)), \"Results do not match!\"\n\n    print(\"Inference successful!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}