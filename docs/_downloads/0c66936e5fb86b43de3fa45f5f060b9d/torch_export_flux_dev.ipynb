{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Compiling FLUX.1-dev model using the Torch-TensorRT dynamo backend\n\nThis example illustrates the state of the art model [FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev) optimized using\nTorch-TensorRT.\n\n**FLUX.1 [dev]** is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions. It is an open-weight, guidance-distilled model for non-commercial applications.\n\nInstall the following dependencies before compilation\n\n```python\npip install sentencepiece==\"0.2.0\" transformers==\"4.48.2\" accelerate==\"1.3.0\" diffusers==\"0.32.2\"\n```\nThere are different components of the ``FLUX.1-dev`` pipeline such as ``transformer``, ``vae``, ``text_encoder``, ``tokenizer`` and ``scheduler``. In this example,\nwe demonstrate optimizing the ``transformer`` component of the model (which typically consumes >95% of the e2e diffusion latency)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import the following libraries\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch_tensorrt\nfrom diffusers import FluxPipeline\nfrom torch.export._trace import _export"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the FLUX-1.dev model\nLoad the ``FLUX-1.dev`` pretrained pipeline using ``FluxPipeline`` class.\n``FluxPipeline`` includes different components such as ``transformer``, ``vae``, ``text_encoder``, ``tokenizer`` and ``scheduler`` necessary\nto generate an image. We load the weights in ``FP16`` precision using ``torch_dtype`` argument\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "DEVICE = \"cuda:0\"\npipe = FluxPipeline.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\",\n    torch_dtype=torch.float16,\n)\npipe.to(DEVICE).to(torch.float16)\n# Store the config and transformer backbone\nconfig = pipe.transformer.config\nbackbone = pipe.transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export the backbone using torch.export\nDefine the dummy inputs and their respective dynamic shapes. We export the transformer backbone with dynamic shapes with a ``batch_size=2``\ndue to [0/1 specialization](https://docs.google.com/document/d/16VPOa3d-Liikf48teAOmxLc92rgvJdfosIy-yoT38Io/edit?fbclid=IwAR3HNwmmexcitV0pbZm_x1a4ykdXZ9th_eJWK-3hBtVgKnrkmemz6Pm5jRQ&tab=t.0#heading=h.ez923tomjvyk)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 2\nBATCH = torch.export.Dim(\"batch\", min=1, max=2)\nSEQ_LEN = torch.export.Dim(\"seq_len\", min=1, max=512)\n# This particular min, max values for img_id input are recommended by torch dynamo during the export of the model.\n# To see this recommendation, you can try exporting using min=1, max=4096\nIMG_ID = torch.export.Dim(\"img_id\", min=3586, max=4096)\ndynamic_shapes = {\n    \"hidden_states\": {0: BATCH},\n    \"encoder_hidden_states\": {0: BATCH, 1: SEQ_LEN},\n    \"pooled_projections\": {0: BATCH},\n    \"timestep\": {0: BATCH},\n    \"txt_ids\": {0: SEQ_LEN},\n    \"img_ids\": {0: IMG_ID},\n    \"guidance\": {0: BATCH},\n}\n# The guidance factor is of type torch.float32\ndummy_inputs = {\n    \"hidden_states\": torch.randn((batch_size, 4096, 64), dtype=torch.float16).to(\n        DEVICE\n    ),\n    \"encoder_hidden_states\": torch.randn(\n        (batch_size, 512, 4096), dtype=torch.float16\n    ).to(DEVICE),\n    \"pooled_projections\": torch.randn((batch_size, 768), dtype=torch.float16).to(\n        DEVICE\n    ),\n    \"timestep\": torch.tensor([1.0, 1.0], dtype=torch.float16).to(DEVICE),\n    \"txt_ids\": torch.randn((512, 3), dtype=torch.float16).to(DEVICE),\n    \"img_ids\": torch.randn((4096, 3), dtype=torch.float16).to(DEVICE),\n    \"guidance\": torch.tensor([1.0, 1.0], dtype=torch.float32).to(DEVICE),\n}\n# This will create an exported program which is going to be compiled with Torch-TensorRT\nep = _export(\n    backbone,\n    args=(),\n    kwargs=dummy_inputs,\n    dynamic_shapes=dynamic_shapes,\n    strict=False,\n    allow_complex_guards_as_runtime_asserts=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Torch-TensorRT compilation\n<div class=\"alert alert-info\"><h4>Note</h4><p>The compilation requires a GPU with high memory (> 80GB) since TensorRT is storing the weights in FP32 precision. This is a known issue and will be resolved in the future.</p></div>\n\n\nWe enable ``FP32`` matmul accumulation using ``use_fp32_acc=True`` to ensure accuracy is preserved by introducing cast to ``FP32`` nodes.\nWe also enable explicit typing to ensure TensorRT respects the datatypes set by the user which is a requirement for FP32 matmul accumulation.\nSince this is a 12 billion parameter model, it takes around 20-30 min to compile on H100 GPU. The model is completely convertible and results in\na single TensorRT engine.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trt_gm = torch_tensorrt.dynamo.compile(\n    ep,\n    inputs=dummy_inputs,\n    enabled_precisions={torch.float32},\n    truncate_double=True,\n    min_block_size=1,\n    use_fp32_acc=True,\n    use_explicit_typing=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post Processing\nRelease the GPU memory occupied by the exported program and the pipe.transformer\nSet the transformer in the Flux pipeline to the Torch-TRT compiled model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "backbone.to(\"cpu\")\ndel ep\npipe.transformer = trt_gm\npipe.transformer.config = config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Image generation using prompt\nProvide a prompt and the file name of the image to be generated. Here we use the\nprompt ``A golden retriever holding a sign to code``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Function which generates images from the flux pipeline\ndef generate_image(pipe, prompt, image_name):\n    seed = 42\n    image = pipe(\n        prompt,\n        output_type=\"pil\",\n        num_inference_steps=20,\n        generator=torch.Generator(\"cuda\").manual_seed(seed),\n    ).images[0]\n    image.save(f\"{image_name}.png\")\n    print(f\"Image generated using {image_name} model saved as {image_name}.png\")\n\n\ngenerate_image(pipe, [\"A golden retriever holding a sign to code\"], \"dog_code\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The generated image is as shown below\n\n<img src=\"file://dog_code.png\">\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}