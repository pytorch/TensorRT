{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Pre-allocated output buffer\n\nThe TensorRT runtime module acts as a wrapper around a PyTorch model (or subgraph) that has been compiled and optimized into a TensorRT engine.\n\nWhen the compiled module is executed, input and output tensors are set to TensorRT context for processing.\nIf output buffer allocation is moved after the execution of the TensorRT context and used it for next inference, GPU tasks and memory allocation tasks can operate concurrently. This overlap allows for more efficient use of GPU resources, potentially improving the performance of inference.\n\nThis optimization is particularly effective in below cases\n\n1. Small inference time\n    - The allocation of output buffers typically requires minimal CPU cycles, as the caching mechanism efficiently handles memory reuse. The time taken for this allocation is relatively constant compared to the overall inference time, leading to noticeable performance improvements, especially in scenarios involving small inference workloads. This is because the reduced allocation time contributes to faster execution when the computational workload is not large enough to overshadow these savings.\n2. Multiple graph breaks\n    - If the module contains operations that are not supported by TensorRT, the unsupported parts are handled by PyTorch and this fallback results in a graph break. The cumulative effect of optimized buffer allocations across multiple subgraphs can enhance overall inference performance.\n    - While optimizing output buffers can mitigate some of this overhead, reducing or removing graph breaks should be prioritized as it enables more comprehensive optimizations\n3. Static input or infrequent input shape change\n    - If shape is changed, pre-allocated buffer cannot be used for next inference and there will new allocation before executing the TensorRT context. This feature is not suitable for use cases with frequent input shape changes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Model Definition\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import timeit\n\nimport numpy as np\nimport torch\nimport torch_tensorrt\nfrom transformers import BertModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define function to measure inference performance\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def test_module_perf(model, *input):\n    timings = []\n\n    # Warm-up phase to ensure consistent and accurate performance measurements.\n    with torch.no_grad():\n        for _ in range(3):\n            model(*input)\n    torch.cuda.synchronize()\n\n    # Timing phase to measure inference performance\n    with torch.no_grad():\n        for i in range(10):\n            start_time = timeit.default_timer()\n            model(*input)\n            torch.cuda.synchronize()\n            end_time = timeit.default_timer()\n            timings.append(end_time - start_time)\n    times = np.array(timings)\n    time_med = np.median(times)\n\n    # Return the median time as a representative performance metric\n    return time_med"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load model and compile\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load bert model\nmodel = (\n    BertModel.from_pretrained(\"bert-base-uncased\", torchscript=True)\n    .eval()\n    .half()\n    .to(\"cuda\")\n)\n# Define sample inputs\ninputs = [\n    torch.randint(0, 5, (1, 128), dtype=torch.int32).to(\"cuda\"),\n    torch.randint(0, 5, (1, 128), dtype=torch.int32).to(\"cuda\"),\n]\n# Next, we compile the model using torch_tensorrt.compile\noptimized_model = torch_tensorrt.compile(\n    model,\n    ir=\"dynamo\",\n    enabled_precisions={torch.half},\n    inputs=inputs,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enable/Disable pre-allocated output buffer feature using runtime api\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Enable pre-allocated output buffer using a context manager\nwith torch_tensorrt.runtime.enable_pre_allocated_outputs(optimized_model):\n    out_trt = optimized_model(*inputs)\n    # Subsequent inferences can use the pre-allocated output buffer (no shape change)\n    out_trt = optimized_model(*inputs)\n\n# Alternatively, we can enable the feature using a context object\npre_allocated_output_ctx = torch_tensorrt.runtime.enable_pre_allocated_outputs(\n    optimized_model\n)\npre_allocated_output_ctx.set_pre_allocated_output(True)\ntime_opt = test_module_perf(optimized_model, *inputs)\n\n# Disable the pre-allocated output buffer feature and perform inference normally\npre_allocated_output_ctx.set_pre_allocated_output(False)\nout_trt = optimized_model(*inputs)\ntime_normal = test_module_perf(optimized_model, *inputs)\n\ntime_opt_ms = time_opt * 1000\ntime_normal_ms = time_normal * 1000\n\nprint(f\"normal trt model time: {time_normal_ms:.3f} ms\")\nprint(f\"pre-allocated output buffer model time: {time_opt_ms:.3f} ms\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}