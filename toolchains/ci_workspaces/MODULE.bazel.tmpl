module(
    name = "torch_tensorrt",
    repo_name = "org_pytorch_tensorrt",
    version = "${BUILD_VERSION}"
)

bazel_dep(name = "googletest", version = "1.16.0")
bazel_dep(name = "platforms", version = "0.0.11")
bazel_dep(name = "rules_cc", version = "0.1.1")
bazel_dep(name = "rules_python", version = "1.3.0")

python = use_extension("@rules_python//python/extensions:python.bzl", "python")
python.toolchain(
    ignore_root_user_error = True,
    python_version = "3.11",
)

bazel_dep(name = "rules_pkg", version = "1.0.1")
git_override(
    module_name = "rules_pkg",
    commit = "17c57f4",
    remote = "https://github.com/narendasan/rules_pkg",
)

local_repository = use_repo_rule("@bazel_tools//tools/build_defs/repo:local.bzl", "local_repository")

# External dependency for torch_tensorrt if you already have precompiled binaries.
local_repository(
    name = "torch_tensorrt",
    path = "/opt/conda/lib/python3.11/site-packages/torch_tensorrt",
)


new_local_repository = use_repo_rule("@bazel_tools//tools/build_defs/repo:local.bzl", "new_local_repository")

# CUDA should be installed on the system locally

new_local_repository(
    name = "cuda",
    build_file = "@//third_party/cuda:BUILD",
    path = "${CUDA_HOME}",
)

# These versions can be selected using the flag `--//toolchains/dep_collection:compute_libs="jetpack"`
new_local_repository(
    name = "cuda_l4t",
    build_file = "@//third_party/cuda:BUILD",
    path = "/usr/local/cuda-12.6",
)

new_local_repository(
    name = "cuda_win",
    build_file = "@//third_party/cuda:BUILD",
    path = "${CUDA_HOME}",
)


http_archive = use_repo_rule("@bazel_tools//tools/build_defs/repo:http.bzl", "http_archive")

#############################################################################################################
# Tarballs and fetched dependencies (default - use in cases when building from precompiled bin and tarballs)
#############################################################################################################

#http_archive(
#    name = "libtorch",
#    build_file = "@//third_party/libtorch:BUILD",
#    strip_prefix = "libtorch",
#    urls = ["https://download.pytorch.org/libtorch/${CHANNEL}/${CU_VERSION}/libtorch-shared-with-deps-latest.zip"],
#)

# Download these tarballs manually from the NVIDIA website
# Either place them in the distdir directory in third_party and use the --distdir flag
# or modify the urls to "file:///<PATH TO TARBALL>/<TARBALL NAME>.tar.gz

http_archive(
    name = "tensorrt",
    build_file = "@//third_party/tensorrt/archive:BUILD",
    strip_prefix = "TensorRT-10.13.3.9",
    urls = [
        "https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.13.3/tars/TensorRT-10.13.3.9.Linux.x86_64-gnu.cuda-${CU_UPPERBOUND}.tar.gz",
    ],
)

http_archive(
    name = "tensorrt_rtx",
    build_file = "@//third_party/tensorrt_rtx/archive:BUILD",
    strip_prefix = "TensorRT-RTX-1.0.0.21",
    urls = [
        "https://developer.nvidia.com/downloads/trt/rtx_sdk/secure/1.0/TensorRT-RTX-1.0.0.21.Linux.x86_64-gnu.cuda-12.9.tar.gz",
    ],
)

http_archive(
    name = "tensorrt_sbsa",
    build_file = "@//third_party/tensorrt/archive:BUILD",
    strip_prefix = "TensorRT-10.13.3.9",
    urls = [
        "https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.13.3/tars/TensorRT-10.13.3.9.Linux.aarch64-gnu.cuda-13.0.tar.gz",
    ],
)

http_archive(
    name = "tensorrt_l4t",
    build_file = "@//third_party/tensorrt/archive:BUILD",
    strip_prefix = "TensorRT-10.3.0.26",
    urls = [
        "https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.3.0/tars/TensorRT-10.3.0.26.l4t.aarch64-gnu.cuda-12.6.tar.gz",
    ],
)

http_archive(
    name = "tensorrt_win",
    build_file = "@//third_party/tensorrt/archive:BUILD",
    strip_prefix = "TensorRT-10.13.3.9",
    urls = [
        "https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.13.3/zip/TensorRT-10.13.3.9.Windows.win10.cuda-${CU_UPPERBOUND}.zip",
    ],
)

http_archive(
    name = "tensorrt_rtx_win",
    build_file = "@//third_party/tensorrt_rtx/archive:BUILD",
    strip_prefix = "TensorRT-RTX-1.0.0.21",
    urls = [
        "https://developer.nvidia.com/downloads/trt/rtx_sdk/secure/1.0/TensorRT-RTX-1.0.0.21.Windows.win10.cuda-12.9.zip",
    ],
)

####################################################################################
# Locally installed dependencies (use in cases of custom dependencies or aarch64)
####################################################################################

# NOTE: If you are using a local build of torch, just point the Libtorch dep to that path.

new_local_repository(
    name = "libtorch",
    path = "${TORCH_INSTALL_PATH}",
    build_file = "third_party/libtorch/BUILD"
)

new_local_repository(
    name = "libtorch_win",
    path = "${TORCH_INSTALL_PATH}",
    build_file = "third_party/libtorch/BUILD"
)

new_local_repository(
    name = "torch_l4t",
    path = "${TORCH_INSTALL_PATH}",
    build_file = "third_party/libtorch/BUILD"
)

#new_local_repository(
#   name = "tensorrt",
#   path = "/usr/",
#   build_file = "@//third_party/tensorrt/local:BUILD"
#)
