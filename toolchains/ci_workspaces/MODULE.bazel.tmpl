module(
    name = "torch_tensorrt",
    repo_name = "org_pytorch_tensorrt",
    version = "${BUILD_VERSION}"
)

bazel_dep(name = "googletest", version = "1.16.0")
bazel_dep(name = "platforms", version = "0.0.11")
bazel_dep(name = "rules_cc", version = "0.1.1")
bazel_dep(name = "rules_python", version = "1.3.0")
bazel_dep(name = "bazel_skylib", version = "1.3.0")

python = use_extension("@rules_python//python/extensions:python.bzl", "python")
python.toolchain(
    ignore_root_user_error = True,
    python_version = "3.11",
)

bazel_dep(name = "rules_pkg", version = "1.0.1")
git_override(
    module_name = "rules_pkg",
    commit = "17c57f4",
    remote = "https://github.com/narendasan/rules_pkg",
)

local_repository = use_repo_rule("@bazel_tools//tools/build_defs/repo:local.bzl", "local_repository")

# External dependency for torch_tensorrt if you already have precompiled binaries.
local_repository(
    name = "torch_tensorrt",
    path = "/opt/conda/lib/python3.11/site-packages/torch_tensorrt",
)


new_local_repository = use_repo_rule("@bazel_tools//tools/build_defs/repo:local.bzl", "new_local_repository")

# CUDA should be installed on the system locally

new_local_repository(
    name = "cuda",
    build_file = "@//third_party/cuda:BUILD",
    path = "${CUDA_HOME}",
)

# These versions can be selected using the flag `--//toolchains/dep_collection:compute_libs="jetpack"`
new_local_repository(
    name = "cuda_l4t",
    build_file = "@//third_party/cuda:BUILD",
    path = "/usr/local/cuda-12.6",
)

new_local_repository(
    name = "cuda_win",
    build_file = "@//third_party/cuda:BUILD",
    path = "${CUDA_HOME}",
)


http_archive = use_repo_rule("@bazel_tools//tools/build_defs/repo:http.bzl", "http_archive")

#############################################################################################################
# Tarballs and fetched dependencies (default - use in cases when building from precompiled bin and tarballs)
#############################################################################################################

#http_archive(
#    name = "libtorch",
#    build_file = "@//third_party/libtorch:BUILD",
#    strip_prefix = "libtorch",
#    urls = ["https://download.pytorch.org/libtorch/${CHANNEL}/${CU_VERSION}/libtorch-shared-with-deps-latest.zip"],
#)

# Download these tarballs manually from the NVIDIA website
# Either place them in the distdir directory in third_party and use the --distdir flag
# or modify the urls to "file:///<PATH TO TARBALL>/<TARBALL NAME>.tar.gz

http_archive(
    name = "tensorrt",
    build_file = "@//third_party/tensorrt/archive:BUILD",
    strip_prefix = "TensorRT-10.11.0.33",
    urls = [
        "https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.11.0/tars/TensorRT-10.11.0.33.Linux.x86_64-gnu.cuda-12.9.tar.gz",
    ],
)

http_archive(
    name = "tensorrt_rtx",
    build_file = "@//third_party/tensorrt_rtx/archive:BUILD",
    sha256 = "b1222e08f9d473f0bcc06c6a76bf2b1327a106dcee671415c4c46833a105a425",
    strip_prefix = "TensorRT-RTX-1.0.0.21",
    urls = [
        "http://cuda-repo/release-candidates/Libraries/TensorRT/v10.12/10.12.0.35-51f47a12/12.9-r575/Linux-x64-manylinux_2_28-winjit/tar/TensorRT-RTX-1.0.0.21.Linux.x86_64-gnu.cuda-12.9.tar.gz",
    ],
)

http_archive(
    name = "tensorrt_sbsa",
    build_file = "@//third_party/tensorrt/archive:BUILD",
    strip_prefix = "TensorRT-10.11.0.33",
    urls = [
        "https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.11.0/tars/TensorRT-10.11.0.33.Linux.aarch64-gnu.cuda-12.9.tar.gz",
    ],
)

http_archive(
    name = "tensorrt_l4t",
    build_file = "@//third_party/tensorrt/archive:BUILD",
    strip_prefix = "TensorRT-10.3.0.26",
    urls = [
        "https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.3.0/tars/TensorRT-10.3.0.26.l4t.aarch64-gnu.cuda-12.6.tar.gz",
    ],
)

http_archive(
    name = "tensorrt_win",
    build_file = "@//third_party/tensorrt/archive:BUILD",
    strip_prefix = "TensorRT-10.11.0.33",
    urls = [
        "https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.11.0/zip/TensorRT-10.11.0.33.Windows.win10.cuda-12.9.zip",
    ],
)

http_archive(
    name = "tensorrt_rtx_win",
    build_file = "@//third_party/tensorrt_rtx/archive:BUILD",
    sha256 = "49cf1247ada75faa8d538257b763b1c12b9bbb97fcd7765654c55b3ad16bd680",
    strip_prefix = "TensorRT-RTX-1.0.0.21",
    urls = [
        "http://cuda-repo/release-candidates/Libraries/TensorRT/v10.12/10.12.0.35-51f47a12/12.9-r575/Windows10-x64-winjit/zip/TensorRT-RTX-1.0.0.21.Windows.win10.cuda-12.9.zip",
    ],
)

####################################################################################
# Locally installed dependencies (use in cases of custom dependencies or aarch64)
####################################################################################

# NOTE: If you are using a local build of torch, just point the Libtorch dep to that path.

new_local_repository(
    name = "libtorch",
    path = "${TORCH_INSTALL_PATH}",
    build_file = "third_party/libtorch/BUILD"
)

new_local_repository(
    name = "libtorch_win",
    path = "${TORCH_INSTALL_PATH}",
    build_file = "third_party/libtorch/BUILD"
)

new_local_repository(
    name = "torch_l4t",
    path = "${TORCH_INSTALL_PATH}",
    build_file = "third_party/libtorch/BUILD"
)

#new_local_repository(
#   name = "tensorrt",
#   path = "/usr/",
#   build_file = "@//third_party/tensorrt/local:BUILD"
#)
